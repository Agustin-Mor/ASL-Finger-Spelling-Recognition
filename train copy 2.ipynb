{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Output layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass: compute predicted y\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Define model parameters\n",
    "input_size = 63\n",
    "hidden_size = 128  # You can adjust this value\n",
    "num_classes = 24\n",
    "\n",
    "# Instantiate the model\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # lr is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "# You would call this function with your data loader\n",
    "# train(model, your_data_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "# Call this with your test data loader\n",
    "# test(model, your_test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>...</th>\n",
       "      <th>z11</th>\n",
       "      <th>z12</th>\n",
       "      <th>z13</th>\n",
       "      <th>z14</th>\n",
       "      <th>z15</th>\n",
       "      <th>z16</th>\n",
       "      <th>z17</th>\n",
       "      <th>z18</th>\n",
       "      <th>z19</th>\n",
       "      <th>z20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>data/images/A/001.jpg</td>\n",
       "      <td>0.588086</td>\n",
       "      <td>0.610797</td>\n",
       "      <td>0.594727</td>\n",
       "      <td>0.589181</td>\n",
       "      <td>0.620655</td>\n",
       "      <td>0.563926</td>\n",
       "      <td>0.560471</td>\n",
       "      <td>0.561863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701379</td>\n",
       "      <td>0.777772</td>\n",
       "      <td>0.836724</td>\n",
       "      <td>0.704725</td>\n",
       "      <td>0.748712</td>\n",
       "      <td>0.801369</td>\n",
       "      <td>0.809782</td>\n",
       "      <td>0.766050</td>\n",
       "      <td>0.759826</td>\n",
       "      <td>0.758081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>data/images/A/002.jpg</td>\n",
       "      <td>0.623728</td>\n",
       "      <td>0.722062</td>\n",
       "      <td>0.783923</td>\n",
       "      <td>0.794917</td>\n",
       "      <td>0.834841</td>\n",
       "      <td>0.741979</td>\n",
       "      <td>0.736999</td>\n",
       "      <td>0.720012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632610</td>\n",
       "      <td>0.722303</td>\n",
       "      <td>0.801031</td>\n",
       "      <td>0.628543</td>\n",
       "      <td>0.714150</td>\n",
       "      <td>0.805167</td>\n",
       "      <td>0.785356</td>\n",
       "      <td>0.697666</td>\n",
       "      <td>0.742781</td>\n",
       "      <td>0.785058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>data/images/A/003.jpg</td>\n",
       "      <td>0.459312</td>\n",
       "      <td>0.492102</td>\n",
       "      <td>0.509328</td>\n",
       "      <td>0.532466</td>\n",
       "      <td>0.549604</td>\n",
       "      <td>0.532722</td>\n",
       "      <td>0.506567</td>\n",
       "      <td>0.482298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661588</td>\n",
       "      <td>0.729322</td>\n",
       "      <td>0.791247</td>\n",
       "      <td>0.704372</td>\n",
       "      <td>0.733751</td>\n",
       "      <td>0.768470</td>\n",
       "      <td>0.785318</td>\n",
       "      <td>0.770730</td>\n",
       "      <td>0.752740</td>\n",
       "      <td>0.741086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>data/images/A/004.jpg</td>\n",
       "      <td>0.435375</td>\n",
       "      <td>0.477092</td>\n",
       "      <td>0.480671</td>\n",
       "      <td>0.489050</td>\n",
       "      <td>0.537486</td>\n",
       "      <td>0.463724</td>\n",
       "      <td>0.454008</td>\n",
       "      <td>0.441640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767333</td>\n",
       "      <td>0.830541</td>\n",
       "      <td>0.911144</td>\n",
       "      <td>0.767338</td>\n",
       "      <td>0.821366</td>\n",
       "      <td>0.865663</td>\n",
       "      <td>0.886155</td>\n",
       "      <td>0.825251</td>\n",
       "      <td>0.835758</td>\n",
       "      <td>0.835150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>data/images/A/005.jpg</td>\n",
       "      <td>0.564398</td>\n",
       "      <td>0.656270</td>\n",
       "      <td>0.711758</td>\n",
       "      <td>0.731674</td>\n",
       "      <td>0.777790</td>\n",
       "      <td>0.675478</td>\n",
       "      <td>0.685107</td>\n",
       "      <td>0.659716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637060</td>\n",
       "      <td>0.723884</td>\n",
       "      <td>0.775409</td>\n",
       "      <td>0.623136</td>\n",
       "      <td>0.704702</td>\n",
       "      <td>0.789816</td>\n",
       "      <td>0.748462</td>\n",
       "      <td>0.677460</td>\n",
       "      <td>0.718147</td>\n",
       "      <td>0.756193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                   path        x0        x1        x2        x3  \\\n",
       "0     A  data/images/A/001.jpg  0.588086  0.610797  0.594727  0.589181   \n",
       "1     A  data/images/A/002.jpg  0.623728  0.722062  0.783923  0.794917   \n",
       "2     A  data/images/A/003.jpg  0.459312  0.492102  0.509328  0.532466   \n",
       "3     A  data/images/A/004.jpg  0.435375  0.477092  0.480671  0.489050   \n",
       "4     A  data/images/A/005.jpg  0.564398  0.656270  0.711758  0.731674   \n",
       "\n",
       "         x4        x5        x6        x7  ...       z11       z12       z13  \\\n",
       "0  0.620655  0.563926  0.560471  0.561863  ...  0.701379  0.777772  0.836724   \n",
       "1  0.834841  0.741979  0.736999  0.720012  ...  0.632610  0.722303  0.801031   \n",
       "2  0.549604  0.532722  0.506567  0.482298  ...  0.661588  0.729322  0.791247   \n",
       "3  0.537486  0.463724  0.454008  0.441640  ...  0.767333  0.830541  0.911144   \n",
       "4  0.777790  0.675478  0.685107  0.659716  ...  0.637060  0.723884  0.775409   \n",
       "\n",
       "        z14       z15       z16       z17       z18       z19       z20  \n",
       "0  0.704725  0.748712  0.801369  0.809782  0.766050  0.759826  0.758081  \n",
       "1  0.628543  0.714150  0.805167  0.785356  0.697666  0.742781  0.785058  \n",
       "2  0.704372  0.733751  0.768470  0.785318  0.770730  0.752740  0.741086  \n",
       "3  0.767338  0.821366  0.865663  0.886155  0.825251  0.835758  0.835150  \n",
       "4  0.623136  0.704702  0.789816  0.748462  0.677460  0.718147  0.756193  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"processed_data.parquet\")\n",
    "\n",
    "column_names = [['label'] + ['path'] + [f'x{x}' for x in range(21)] + [f'y{x}' for x in range(21)] + [f'z{x}' for x in range(21)]]\n",
    "df.columns = column_names\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# le = LabelEncoder()\n",
    "\n",
    "# # Fit and transform your DataFrame's last column\n",
    "# df['label'] = le.fit_transform(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the data into features (X) and labels (y)\n",
    "# X = df.iloc[:, :-1]  # First 63 columns as features\n",
    "# y = df['label']  # Last column as the label\n",
    "\n",
    "# # Split the dataset into training set and test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# # Convert to tensors\n",
    "# train_tensors = TensorDataset(torch.tensor(X_train.values, dtype=torch.float), torch.tensor(y_train.values, dtype=torch.long))\n",
    "# test_tensors = TensorDataset(torch.tensor(X_test.values, dtype=torch.float), torch.tensor(y_test.values, dtype=torch.long))\n",
    "\n",
    "# # Create DataLoaders\n",
    "# batch_size = 32  # Choose according to your needs or performance considerations\n",
    "# train_loader = DataLoader(train_tensors, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_tensors, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agustin/.local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected sequence or array-like, got <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Split features and target\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# X = df.iloc[:, :63].values  # Features\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# y = df['label'].values  # Target\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Split the dataset into train and test\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Convert to float32 for better compatibility with PyTorch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2777\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2773\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2775\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m-> 2777\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[43m_num_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2778\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[1;32m   2779\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[1;32m   2780\u001b[0m )\n\u001b[1;32m   2782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:381\u001b[0m, in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    379\u001b[0m         x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(message)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Convert output strings to integers\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "# Split features and target\n",
    "# X = df.iloc[:, :63].values  # Features\n",
    "# y = df['label'].values  # Target\n",
    "\n",
    "# Split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(None, None, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to float32 for better compatibility with PyTorch\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y).long()  # Long tensor for classification labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/6612 (0%)]\tLoss: 3.180721\n",
      "Train Epoch: 0 [3200/6612 (48%)]\tLoss: 3.147403\n",
      "Train Epoch: 0 [6400/6612 (97%)]\tLoss: 2.978036\n",
      "Train Epoch: 1 [0/6612 (0%)]\tLoss: 2.932304\n",
      "Train Epoch: 1 [3200/6612 (48%)]\tLoss: 2.796500\n",
      "Train Epoch: 1 [6400/6612 (97%)]\tLoss: 2.452566\n",
      "Train Epoch: 2 [0/6612 (0%)]\tLoss: 2.489521\n",
      "Train Epoch: 2 [3200/6612 (48%)]\tLoss: 2.392346\n",
      "Train Epoch: 2 [6400/6612 (97%)]\tLoss: 2.207166\n",
      "Train Epoch: 3 [0/6612 (0%)]\tLoss: 2.013390\n",
      "Train Epoch: 3 [3200/6612 (48%)]\tLoss: 1.849828\n",
      "Train Epoch: 3 [6400/6612 (97%)]\tLoss: 1.876013\n",
      "Train Epoch: 4 [0/6612 (0%)]\tLoss: 1.891037\n",
      "Train Epoch: 4 [3200/6612 (48%)]\tLoss: 1.749223\n",
      "Train Epoch: 4 [6400/6612 (97%)]\tLoss: 1.522879\n",
      "Train Epoch: 5 [0/6612 (0%)]\tLoss: 1.310601\n",
      "Train Epoch: 5 [3200/6612 (48%)]\tLoss: 1.491680\n",
      "Train Epoch: 5 [6400/6612 (97%)]\tLoss: 1.492205\n",
      "Train Epoch: 6 [0/6612 (0%)]\tLoss: 1.482730\n",
      "Train Epoch: 6 [3200/6612 (48%)]\tLoss: 1.011822\n",
      "Train Epoch: 6 [6400/6612 (97%)]\tLoss: 1.216321\n",
      "Train Epoch: 7 [0/6612 (0%)]\tLoss: 1.458319\n",
      "Train Epoch: 7 [3200/6612 (48%)]\tLoss: 1.239582\n",
      "Train Epoch: 7 [6400/6612 (97%)]\tLoss: 0.970049\n",
      "Train Epoch: 8 [0/6612 (0%)]\tLoss: 1.288114\n",
      "Train Epoch: 8 [3200/6612 (48%)]\tLoss: 0.963054\n",
      "Train Epoch: 8 [6400/6612 (97%)]\tLoss: 0.946900\n",
      "Train Epoch: 9 [0/6612 (0%)]\tLoss: 1.039399\n",
      "Train Epoch: 9 [3200/6612 (48%)]\tLoss: 0.904701\n",
      "Train Epoch: 9 [6400/6612 (97%)]\tLoss: 1.005415\n",
      "Train Epoch: 10 [0/6612 (0%)]\tLoss: 1.227701\n",
      "Train Epoch: 10 [3200/6612 (48%)]\tLoss: 0.807646\n",
      "Train Epoch: 10 [6400/6612 (97%)]\tLoss: 0.950833\n",
      "Train Epoch: 11 [0/6612 (0%)]\tLoss: 0.799223\n",
      "Train Epoch: 11 [3200/6612 (48%)]\tLoss: 0.765727\n",
      "Train Epoch: 11 [6400/6612 (97%)]\tLoss: 0.863585\n",
      "Train Epoch: 12 [0/6612 (0%)]\tLoss: 0.796985\n",
      "Train Epoch: 12 [3200/6612 (48%)]\tLoss: 0.883854\n",
      "Train Epoch: 12 [6400/6612 (97%)]\tLoss: 0.823174\n",
      "Train Epoch: 13 [0/6612 (0%)]\tLoss: 0.806330\n",
      "Train Epoch: 13 [3200/6612 (48%)]\tLoss: 0.630436\n",
      "Train Epoch: 13 [6400/6612 (97%)]\tLoss: 0.930521\n",
      "Train Epoch: 14 [0/6612 (0%)]\tLoss: 0.573791\n",
      "Train Epoch: 14 [3200/6612 (48%)]\tLoss: 0.726483\n",
      "Train Epoch: 14 [6400/6612 (97%)]\tLoss: 1.014122\n",
      "Train Epoch: 15 [0/6612 (0%)]\tLoss: 0.602339\n",
      "Train Epoch: 15 [3200/6612 (48%)]\tLoss: 0.729616\n",
      "Train Epoch: 15 [6400/6612 (97%)]\tLoss: 0.547085\n",
      "Train Epoch: 16 [0/6612 (0%)]\tLoss: 0.601702\n",
      "Train Epoch: 16 [3200/6612 (48%)]\tLoss: 0.616434\n",
      "Train Epoch: 16 [6400/6612 (97%)]\tLoss: 0.581588\n",
      "Train Epoch: 17 [0/6612 (0%)]\tLoss: 0.537485\n",
      "Train Epoch: 17 [3200/6612 (48%)]\tLoss: 0.666284\n",
      "Train Epoch: 17 [6400/6612 (97%)]\tLoss: 0.527416\n",
      "Train Epoch: 18 [0/6612 (0%)]\tLoss: 0.463570\n",
      "Train Epoch: 18 [3200/6612 (48%)]\tLoss: 0.573963\n",
      "Train Epoch: 18 [6400/6612 (97%)]\tLoss: 0.713734\n",
      "Train Epoch: 19 [0/6612 (0%)]\tLoss: 0.536255\n",
      "Train Epoch: 19 [3200/6612 (48%)]\tLoss: 0.560625\n",
      "Train Epoch: 19 [6400/6612 (97%)]\tLoss: 0.726373\n",
      "Train Epoch: 20 [0/6612 (0%)]\tLoss: 0.798479\n",
      "Train Epoch: 20 [3200/6612 (48%)]\tLoss: 0.529538\n",
      "Train Epoch: 20 [6400/6612 (97%)]\tLoss: 0.515047\n",
      "Train Epoch: 21 [0/6612 (0%)]\tLoss: 0.523315\n",
      "Train Epoch: 21 [3200/6612 (48%)]\tLoss: 0.311980\n",
      "Train Epoch: 21 [6400/6612 (97%)]\tLoss: 0.278792\n",
      "Train Epoch: 22 [0/6612 (0%)]\tLoss: 0.676931\n",
      "Train Epoch: 22 [3200/6612 (48%)]\tLoss: 0.505979\n",
      "Train Epoch: 22 [6400/6612 (97%)]\tLoss: 0.419116\n",
      "Train Epoch: 23 [0/6612 (0%)]\tLoss: 0.535980\n",
      "Train Epoch: 23 [3200/6612 (48%)]\tLoss: 0.596478\n",
      "Train Epoch: 23 [6400/6612 (97%)]\tLoss: 0.639085\n",
      "Train Epoch: 24 [0/6612 (0%)]\tLoss: 0.450607\n",
      "Train Epoch: 24 [3200/6612 (48%)]\tLoss: 0.803057\n",
      "Train Epoch: 24 [6400/6612 (97%)]\tLoss: 0.429955\n",
      "Train Epoch: 25 [0/6612 (0%)]\tLoss: 0.394152\n",
      "Train Epoch: 25 [3200/6612 (48%)]\tLoss: 0.360133\n",
      "Train Epoch: 25 [6400/6612 (97%)]\tLoss: 0.402493\n",
      "Train Epoch: 26 [0/6612 (0%)]\tLoss: 0.457511\n",
      "Train Epoch: 26 [3200/6612 (48%)]\tLoss: 0.323929\n",
      "Train Epoch: 26 [6400/6612 (97%)]\tLoss: 0.559826\n",
      "Train Epoch: 27 [0/6612 (0%)]\tLoss: 0.540442\n",
      "Train Epoch: 27 [3200/6612 (48%)]\tLoss: 0.423304\n",
      "Train Epoch: 27 [6400/6612 (97%)]\tLoss: 0.848913\n",
      "Train Epoch: 28 [0/6612 (0%)]\tLoss: 0.413335\n",
      "Train Epoch: 28 [3200/6612 (48%)]\tLoss: 0.370922\n",
      "Train Epoch: 28 [6400/6612 (97%)]\tLoss: 0.317578\n",
      "Train Epoch: 29 [0/6612 (0%)]\tLoss: 0.448343\n",
      "Train Epoch: 29 [3200/6612 (48%)]\tLoss: 0.514246\n",
      "Train Epoch: 29 [6400/6612 (97%)]\tLoss: 0.592689\n",
      "Train Epoch: 30 [0/6612 (0%)]\tLoss: 0.356518\n",
      "Train Epoch: 30 [3200/6612 (48%)]\tLoss: 0.264500\n",
      "Train Epoch: 30 [6400/6612 (97%)]\tLoss: 0.277271\n",
      "Train Epoch: 31 [0/6612 (0%)]\tLoss: 0.577530\n",
      "Train Epoch: 31 [3200/6612 (48%)]\tLoss: 0.351537\n",
      "Train Epoch: 31 [6400/6612 (97%)]\tLoss: 0.492261\n",
      "Train Epoch: 32 [0/6612 (0%)]\tLoss: 0.218195\n",
      "Train Epoch: 32 [3200/6612 (48%)]\tLoss: 0.523861\n",
      "Train Epoch: 32 [6400/6612 (97%)]\tLoss: 0.433965\n",
      "Train Epoch: 33 [0/6612 (0%)]\tLoss: 0.345740\n",
      "Train Epoch: 33 [3200/6612 (48%)]\tLoss: 0.732093\n",
      "Train Epoch: 33 [6400/6612 (97%)]\tLoss: 0.312479\n",
      "Train Epoch: 34 [0/6612 (0%)]\tLoss: 0.342780\n",
      "Train Epoch: 34 [3200/6612 (48%)]\tLoss: 0.378688\n",
      "Train Epoch: 34 [6400/6612 (97%)]\tLoss: 0.316763\n",
      "Train Epoch: 35 [0/6612 (0%)]\tLoss: 0.188696\n",
      "Train Epoch: 35 [3200/6612 (48%)]\tLoss: 0.652097\n",
      "Train Epoch: 35 [6400/6612 (97%)]\tLoss: 0.426524\n",
      "Train Epoch: 36 [0/6612 (0%)]\tLoss: 0.466163\n",
      "Train Epoch: 36 [3200/6612 (48%)]\tLoss: 0.178495\n",
      "Train Epoch: 36 [6400/6612 (97%)]\tLoss: 0.295461\n",
      "Train Epoch: 37 [0/6612 (0%)]\tLoss: 0.187747\n",
      "Train Epoch: 37 [3200/6612 (48%)]\tLoss: 0.277729\n",
      "Train Epoch: 37 [6400/6612 (97%)]\tLoss: 0.622939\n",
      "Train Epoch: 38 [0/6612 (0%)]\tLoss: 0.287293\n",
      "Train Epoch: 38 [3200/6612 (48%)]\tLoss: 0.250649\n",
      "Train Epoch: 38 [6400/6612 (97%)]\tLoss: 0.389021\n",
      "Train Epoch: 39 [0/6612 (0%)]\tLoss: 0.574201\n",
      "Train Epoch: 39 [3200/6612 (48%)]\tLoss: 0.272417\n",
      "Train Epoch: 39 [6400/6612 (97%)]\tLoss: 0.323293\n",
      "\n",
      "Test set: Average loss: 0.0116, Accuracy: 1494/1654 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, 40)\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.iloc[0].tolist()[:-1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.argmax(model(torch.Tensor(df.iloc[0].tolist()[:-1])).detach().numpy())\n",
    "# model(torch.Tensor(df.iloc[0].tolist()[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = pd.read_json('test.json').iloc[0].to_list()\n",
    "np.argmax(model(torch.Tensor(inp)).detach().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
