{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Output layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass: compute predicted y\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Define model parameters\n",
    "input_size = 63\n",
    "hidden_size = 128  # You can adjust this value\n",
    "num_classes = 2\n",
    "\n",
    "# Instantiate the model\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # lr is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "# You would call this function with your data loader\n",
    "# train(model, your_data_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "# Call this with your test data loader\n",
    "# test(model, your_test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11113/2288009784.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['label'] = df['label'].replace({'A': 0, 'B': 1})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>z12</th>\n",
       "      <th>z13</th>\n",
       "      <th>z14</th>\n",
       "      <th>z15</th>\n",
       "      <th>z16</th>\n",
       "      <th>z17</th>\n",
       "      <th>z18</th>\n",
       "      <th>z19</th>\n",
       "      <th>z20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.226127</td>\n",
       "      <td>0.234860</td>\n",
       "      <td>0.228681</td>\n",
       "      <td>0.226548</td>\n",
       "      <td>0.238650</td>\n",
       "      <td>0.216837</td>\n",
       "      <td>0.215509</td>\n",
       "      <td>0.216044</td>\n",
       "      <td>0.217245</td>\n",
       "      <td>0.210701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226009</td>\n",
       "      <td>0.243140</td>\n",
       "      <td>0.204783</td>\n",
       "      <td>0.217565</td>\n",
       "      <td>0.232866</td>\n",
       "      <td>0.235311</td>\n",
       "      <td>0.222603</td>\n",
       "      <td>0.220795</td>\n",
       "      <td>0.220288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.198414</td>\n",
       "      <td>0.229695</td>\n",
       "      <td>0.249374</td>\n",
       "      <td>0.252871</td>\n",
       "      <td>0.265572</td>\n",
       "      <td>0.236031</td>\n",
       "      <td>0.234447</td>\n",
       "      <td>0.229043</td>\n",
       "      <td>0.227424</td>\n",
       "      <td>0.219662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229932</td>\n",
       "      <td>0.254994</td>\n",
       "      <td>0.200085</td>\n",
       "      <td>0.227337</td>\n",
       "      <td>0.256310</td>\n",
       "      <td>0.250004</td>\n",
       "      <td>0.222089</td>\n",
       "      <td>0.236451</td>\n",
       "      <td>0.249909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.207967</td>\n",
       "      <td>0.222814</td>\n",
       "      <td>0.230614</td>\n",
       "      <td>0.241090</td>\n",
       "      <td>0.248850</td>\n",
       "      <td>0.241206</td>\n",
       "      <td>0.229363</td>\n",
       "      <td>0.218375</td>\n",
       "      <td>0.213111</td>\n",
       "      <td>0.235535</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227831</td>\n",
       "      <td>0.247175</td>\n",
       "      <td>0.220036</td>\n",
       "      <td>0.229214</td>\n",
       "      <td>0.240060</td>\n",
       "      <td>0.245323</td>\n",
       "      <td>0.240766</td>\n",
       "      <td>0.235146</td>\n",
       "      <td>0.231505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.215926</td>\n",
       "      <td>0.236616</td>\n",
       "      <td>0.238391</td>\n",
       "      <td>0.242547</td>\n",
       "      <td>0.266568</td>\n",
       "      <td>0.229986</td>\n",
       "      <td>0.225167</td>\n",
       "      <td>0.219033</td>\n",
       "      <td>0.218083</td>\n",
       "      <td>0.219858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223613</td>\n",
       "      <td>0.245314</td>\n",
       "      <td>0.206596</td>\n",
       "      <td>0.221143</td>\n",
       "      <td>0.233069</td>\n",
       "      <td>0.238586</td>\n",
       "      <td>0.222188</td>\n",
       "      <td>0.225017</td>\n",
       "      <td>0.224854</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.194080</td>\n",
       "      <td>0.225672</td>\n",
       "      <td>0.244753</td>\n",
       "      <td>0.251601</td>\n",
       "      <td>0.267459</td>\n",
       "      <td>0.232277</td>\n",
       "      <td>0.235588</td>\n",
       "      <td>0.226857</td>\n",
       "      <td>0.221222</td>\n",
       "      <td>0.217419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232845</td>\n",
       "      <td>0.249419</td>\n",
       "      <td>0.200438</td>\n",
       "      <td>0.226675</td>\n",
       "      <td>0.254053</td>\n",
       "      <td>0.240751</td>\n",
       "      <td>0.217912</td>\n",
       "      <td>0.230999</td>\n",
       "      <td>0.243238</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0  0.226127  0.234860  0.228681  0.226548  0.238650  0.216837  0.215509   \n",
       "1  0.198414  0.229695  0.249374  0.252871  0.265572  0.236031  0.234447   \n",
       "2  0.207967  0.222814  0.230614  0.241090  0.248850  0.241206  0.229363   \n",
       "3  0.215926  0.236616  0.238391  0.242547  0.266568  0.229986  0.225167   \n",
       "4  0.194080  0.225672  0.244753  0.251601  0.267459  0.232277  0.235588   \n",
       "\n",
       "         x7        x8        x9  ...       z12       z13       z14       z15  \\\n",
       "0  0.216044  0.217245  0.210701  ...  0.226009  0.243140  0.204783  0.217565   \n",
       "1  0.229043  0.227424  0.219662  ...  0.229932  0.254994  0.200085  0.227337   \n",
       "2  0.218375  0.213111  0.235535  ...  0.227831  0.247175  0.220036  0.229214   \n",
       "3  0.219033  0.218083  0.219858  ...  0.223613  0.245314  0.206596  0.221143   \n",
       "4  0.226857  0.221222  0.217419  ...  0.232845  0.249419  0.200438  0.226675   \n",
       "\n",
       "        z16       z17       z18       z19       z20  label  \n",
       "0  0.232866  0.235311  0.222603  0.220795  0.220288      0  \n",
       "1  0.256310  0.250004  0.222089  0.236451  0.249909      0  \n",
       "2  0.240060  0.245323  0.240766  0.235146  0.231505      0  \n",
       "3  0.233069  0.238586  0.222188  0.225017  0.224854      0  \n",
       "4  0.254053  0.240751  0.217912  0.230999  0.243238      0  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"processed_data.json\")\n",
    "df['label'] = df['label'].replace({'A': 0, 'B': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, dataframe, feature_cols, label_col, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.feature_cols = feature_cols\n",
    "        self.label_col = label_col\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        features = row[self.feature_cols].values.astype(float)\n",
    "        label = row[self.label_col]\n",
    "        \n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        \n",
    "        return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Example usage\n",
    "feature_cols = [f\"x{i}\" for i in range(21)] + [f\"y{i}\" for i in range(21)] + [f\"z{i}\" for i in range(21)]\n",
    "label_col = 'label'\n",
    "dataset = PandasDataset(df, feature_cols, label_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=100, shuffle=True, num_workers=4)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "# Compute lengths\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DataLoader parameters\n",
    "batch_size = 8\n",
    "num_workers = 0  # Adjust based on your system\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/852 (0%)]\tLoss: 0.687674\n",
      "Train Epoch: 0 [800/852 (93%)]\tLoss: 0.453562\n",
      "Train Epoch: 1 [0/852 (0%)]\tLoss: 0.353410\n",
      "Train Epoch: 1 [800/852 (93%)]\tLoss: 0.117124\n",
      "Train Epoch: 2 [0/852 (0%)]\tLoss: 0.043202\n",
      "Train Epoch: 2 [800/852 (93%)]\tLoss: 0.042902\n",
      "Train Epoch: 3 [0/852 (0%)]\tLoss: 0.060777\n",
      "Train Epoch: 3 [800/852 (93%)]\tLoss: 0.017108\n",
      "Train Epoch: 4 [0/852 (0%)]\tLoss: 0.015590\n",
      "Train Epoch: 4 [800/852 (93%)]\tLoss: 0.006397\n",
      "Train Epoch: 5 [0/852 (0%)]\tLoss: 0.026039\n",
      "Train Epoch: 5 [800/852 (93%)]\tLoss: 0.003972\n",
      "Train Epoch: 6 [0/852 (0%)]\tLoss: 0.021379\n",
      "Train Epoch: 6 [800/852 (93%)]\tLoss: 0.017435\n",
      "Train Epoch: 7 [0/852 (0%)]\tLoss: 0.005215\n",
      "Train Epoch: 7 [800/852 (93%)]\tLoss: 0.006607\n",
      "Train Epoch: 8 [0/852 (0%)]\tLoss: 0.008899\n",
      "Train Epoch: 8 [800/852 (93%)]\tLoss: 0.002138\n",
      "Train Epoch: 9 [0/852 (0%)]\tLoss: 0.014611\n",
      "Train Epoch: 9 [800/852 (93%)]\tLoss: 0.004703\n",
      "Train Epoch: 10 [0/852 (0%)]\tLoss: 0.002753\n",
      "Train Epoch: 10 [800/852 (93%)]\tLoss: 0.001391\n",
      "Train Epoch: 11 [0/852 (0%)]\tLoss: 0.002323\n",
      "Train Epoch: 11 [800/852 (93%)]\tLoss: 0.000298\n",
      "Train Epoch: 12 [0/852 (0%)]\tLoss: 0.004463\n",
      "Train Epoch: 12 [800/852 (93%)]\tLoss: 0.003520\n",
      "Train Epoch: 13 [0/852 (0%)]\tLoss: 0.001160\n",
      "Train Epoch: 13 [800/852 (93%)]\tLoss: 0.000960\n",
      "Train Epoch: 14 [0/852 (0%)]\tLoss: 0.000836\n",
      "Train Epoch: 14 [800/852 (93%)]\tLoss: 0.003094\n",
      "Train Epoch: 15 [0/852 (0%)]\tLoss: 0.000858\n",
      "Train Epoch: 15 [800/852 (93%)]\tLoss: 0.000150\n",
      "Train Epoch: 16 [0/852 (0%)]\tLoss: 0.000810\n",
      "Train Epoch: 16 [800/852 (93%)]\tLoss: 0.010424\n",
      "Train Epoch: 17 [0/852 (0%)]\tLoss: 0.000627\n",
      "Train Epoch: 17 [800/852 (93%)]\tLoss: 0.000934\n",
      "Train Epoch: 18 [0/852 (0%)]\tLoss: 0.000325\n",
      "Train Epoch: 18 [800/852 (93%)]\tLoss: 0.000158\n",
      "Train Epoch: 19 [0/852 (0%)]\tLoss: 0.001603\n",
      "Train Epoch: 19 [800/852 (93%)]\tLoss: 0.000815\n",
      "Train Epoch: 20 [0/852 (0%)]\tLoss: 0.001321\n",
      "Train Epoch: 20 [800/852 (93%)]\tLoss: 0.000141\n",
      "Train Epoch: 21 [0/852 (0%)]\tLoss: 0.000079\n",
      "Train Epoch: 21 [800/852 (93%)]\tLoss: 0.000128\n",
      "Train Epoch: 22 [0/852 (0%)]\tLoss: 0.004180\n",
      "Train Epoch: 22 [800/852 (93%)]\tLoss: 0.000207\n",
      "Train Epoch: 23 [0/852 (0%)]\tLoss: 0.001263\n",
      "Train Epoch: 23 [800/852 (93%)]\tLoss: 0.000024\n",
      "Train Epoch: 24 [0/852 (0%)]\tLoss: 0.004105\n",
      "Train Epoch: 24 [800/852 (93%)]\tLoss: 0.000562\n",
      "Train Epoch: 25 [0/852 (0%)]\tLoss: 0.000438\n",
      "Train Epoch: 25 [800/852 (93%)]\tLoss: 0.000285\n",
      "Train Epoch: 26 [0/852 (0%)]\tLoss: 0.001296\n",
      "Train Epoch: 26 [800/852 (93%)]\tLoss: 0.000009\n",
      "Train Epoch: 27 [0/852 (0%)]\tLoss: 0.000184\n",
      "Train Epoch: 27 [800/852 (93%)]\tLoss: 0.001928\n",
      "Train Epoch: 28 [0/852 (0%)]\tLoss: 0.000041\n",
      "Train Epoch: 28 [800/852 (93%)]\tLoss: 0.000311\n",
      "Train Epoch: 29 [0/852 (0%)]\tLoss: 0.000758\n",
      "Train Epoch: 29 [800/852 (93%)]\tLoss: 0.103497\n",
      "Train Epoch: 30 [0/852 (0%)]\tLoss: 0.000180\n",
      "Train Epoch: 30 [800/852 (93%)]\tLoss: 0.000253\n",
      "Train Epoch: 31 [0/852 (0%)]\tLoss: 0.000268\n",
      "Train Epoch: 31 [800/852 (93%)]\tLoss: 0.002597\n",
      "Train Epoch: 32 [0/852 (0%)]\tLoss: 0.000171\n",
      "Train Epoch: 32 [800/852 (93%)]\tLoss: 0.003784\n",
      "Train Epoch: 33 [0/852 (0%)]\tLoss: 0.000930\n",
      "Train Epoch: 33 [800/852 (93%)]\tLoss: 0.000097\n",
      "Train Epoch: 34 [0/852 (0%)]\tLoss: 0.001769\n",
      "Train Epoch: 34 [800/852 (93%)]\tLoss: 0.000048\n",
      "Train Epoch: 35 [0/852 (0%)]\tLoss: 0.000005\n",
      "Train Epoch: 35 [800/852 (93%)]\tLoss: 0.000032\n",
      "Train Epoch: 36 [0/852 (0%)]\tLoss: 0.000081\n",
      "Train Epoch: 36 [800/852 (93%)]\tLoss: 0.000826\n",
      "Train Epoch: 37 [0/852 (0%)]\tLoss: 0.000041\n",
      "Train Epoch: 37 [800/852 (93%)]\tLoss: 0.013698\n",
      "Train Epoch: 38 [0/852 (0%)]\tLoss: 0.000010\n",
      "Train Epoch: 38 [800/852 (93%)]\tLoss: 0.000069\n",
      "Train Epoch: 39 [0/852 (0%)]\tLoss: 0.000036\n",
      "Train Epoch: 39 [800/852 (93%)]\tLoss: 0.000016\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 214/214 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiClassModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassModel, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(63, 128)  # input layer (63) -> hidden layer (128)\n",
    "        self.fc2 = nn.Linear(128, 24)   # hidden layer (128) -> output layer (24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        x = F.relu(self.fc1(x))      # activation function for hidden layer\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)  # log softmax for output\n",
    "\n",
    "# Instantiate the model\n",
    "model = MultiClassModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/852 (0%)]\tLoss: 3.274391\n",
      "Train Epoch: 1 [800/852 (93%)]\tLoss: 0.634232\n",
      "Train Epoch: 2 [0/852 (0%)]\tLoss: 0.592219\n",
      "Train Epoch: 2 [800/852 (93%)]\tLoss: 0.434054\n",
      "Train Epoch: 3 [0/852 (0%)]\tLoss: 0.320384\n",
      "Train Epoch: 3 [800/852 (93%)]\tLoss: 0.130343\n",
      "Train Epoch: 4 [0/852 (0%)]\tLoss: 0.191026\n",
      "Train Epoch: 4 [800/852 (93%)]\tLoss: 0.114458\n",
      "Train Epoch: 5 [0/852 (0%)]\tLoss: 0.119213\n",
      "Train Epoch: 5 [800/852 (93%)]\tLoss: 0.050240\n",
      "Train Epoch: 6 [0/852 (0%)]\tLoss: 0.057566\n",
      "Train Epoch: 6 [800/852 (93%)]\tLoss: 0.027130\n",
      "Train Epoch: 7 [0/852 (0%)]\tLoss: 0.038113\n",
      "Train Epoch: 7 [800/852 (93%)]\tLoss: 0.037230\n",
      "Train Epoch: 8 [0/852 (0%)]\tLoss: 0.077610\n",
      "Train Epoch: 8 [800/852 (93%)]\tLoss: 0.024302\n",
      "Train Epoch: 9 [0/852 (0%)]\tLoss: 0.006672\n",
      "Train Epoch: 9 [800/852 (93%)]\tLoss: 0.013514\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, device, loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(loader.dataset),\n",
    "                100. * batch_idx / len(loader), loss.item()))\n",
    "\n",
    "# Then you would call train in a loop for each epoch\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "model.to('cpu')\n",
    "\n",
    "for epoch in range(1, 10):  # loop over the dataset multiple times\n",
    "    train(model, device, train_loader, optimizer, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
