{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        # Output layer\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass: compute predicted y\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Define model parameters\n",
    "input_size = 63\n",
    "hidden_size = 128  # You can adjust this value\n",
    "num_classes = 24\n",
    "\n",
    "# Instantiate the model\n",
    "model = NeuralNet(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # lr is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "# You would call this function with your data loader\n",
    "# train(model, your_data_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "# Call this with your test data loader\n",
    "# test(model, your_test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>z12</th>\n",
       "      <th>z13</th>\n",
       "      <th>z14</th>\n",
       "      <th>z15</th>\n",
       "      <th>z16</th>\n",
       "      <th>z17</th>\n",
       "      <th>z18</th>\n",
       "      <th>z19</th>\n",
       "      <th>z20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.174138</td>\n",
       "      <td>0.203343</td>\n",
       "      <td>0.229114</td>\n",
       "      <td>0.245728</td>\n",
       "      <td>0.246139</td>\n",
       "      <td>0.236828</td>\n",
       "      <td>0.235357</td>\n",
       "      <td>0.228395</td>\n",
       "      <td>0.228840</td>\n",
       "      <td>0.213760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231463</td>\n",
       "      <td>0.248762</td>\n",
       "      <td>0.233060</td>\n",
       "      <td>0.236895</td>\n",
       "      <td>0.240368</td>\n",
       "      <td>0.222757</td>\n",
       "      <td>0.217457</td>\n",
       "      <td>0.222375</td>\n",
       "      <td>0.230399</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.154908</td>\n",
       "      <td>0.199391</td>\n",
       "      <td>0.247105</td>\n",
       "      <td>0.268197</td>\n",
       "      <td>0.270157</td>\n",
       "      <td>0.253825</td>\n",
       "      <td>0.249148</td>\n",
       "      <td>0.234938</td>\n",
       "      <td>0.229938</td>\n",
       "      <td>0.222525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230862</td>\n",
       "      <td>0.253146</td>\n",
       "      <td>0.231245</td>\n",
       "      <td>0.239760</td>\n",
       "      <td>0.248545</td>\n",
       "      <td>0.227390</td>\n",
       "      <td>0.216762</td>\n",
       "      <td>0.227295</td>\n",
       "      <td>0.241564</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.159652</td>\n",
       "      <td>0.191880</td>\n",
       "      <td>0.228607</td>\n",
       "      <td>0.257717</td>\n",
       "      <td>0.252847</td>\n",
       "      <td>0.257458</td>\n",
       "      <td>0.247691</td>\n",
       "      <td>0.231889</td>\n",
       "      <td>0.229773</td>\n",
       "      <td>0.232824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230438</td>\n",
       "      <td>0.250651</td>\n",
       "      <td>0.238889</td>\n",
       "      <td>0.240883</td>\n",
       "      <td>0.241257</td>\n",
       "      <td>0.225991</td>\n",
       "      <td>0.223449</td>\n",
       "      <td>0.227114</td>\n",
       "      <td>0.233759</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.164240</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.234502</td>\n",
       "      <td>0.259028</td>\n",
       "      <td>0.267355</td>\n",
       "      <td>0.246959</td>\n",
       "      <td>0.244995</td>\n",
       "      <td>0.234087</td>\n",
       "      <td>0.236546</td>\n",
       "      <td>0.217758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231807</td>\n",
       "      <td>0.248318</td>\n",
       "      <td>0.233374</td>\n",
       "      <td>0.239115</td>\n",
       "      <td>0.242942</td>\n",
       "      <td>0.223314</td>\n",
       "      <td>0.217289</td>\n",
       "      <td>0.224675</td>\n",
       "      <td>0.234197</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.151370</td>\n",
       "      <td>0.195552</td>\n",
       "      <td>0.242214</td>\n",
       "      <td>0.266695</td>\n",
       "      <td>0.271002</td>\n",
       "      <td>0.249770</td>\n",
       "      <td>0.250443</td>\n",
       "      <td>0.233929</td>\n",
       "      <td>0.226981</td>\n",
       "      <td>0.219453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232041</td>\n",
       "      <td>0.251699</td>\n",
       "      <td>0.231547</td>\n",
       "      <td>0.239487</td>\n",
       "      <td>0.247135</td>\n",
       "      <td>0.224706</td>\n",
       "      <td>0.215417</td>\n",
       "      <td>0.225201</td>\n",
       "      <td>0.238368</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0  0.174138  0.203343  0.229114  0.245728  0.246139  0.236828  0.235357   \n",
       "1  0.154908  0.199391  0.247105  0.268197  0.270157  0.253825  0.249148   \n",
       "2  0.159652  0.191880  0.228607  0.257717  0.252847  0.257458  0.247691   \n",
       "3  0.164240  0.201327  0.234502  0.259028  0.267355  0.246959  0.244995   \n",
       "4  0.151370  0.195552  0.242214  0.266695  0.271002  0.249770  0.250443   \n",
       "\n",
       "         x7        x8        x9  ...       z12       z13       z14       z15  \\\n",
       "0  0.228395  0.228840  0.213760  ...  0.231463  0.248762  0.233060  0.236895   \n",
       "1  0.234938  0.229938  0.222525  ...  0.230862  0.253146  0.231245  0.239760   \n",
       "2  0.231889  0.229773  0.232824  ...  0.230438  0.250651  0.238889  0.240883   \n",
       "3  0.234087  0.236546  0.217758  ...  0.231807  0.248318  0.233374  0.239115   \n",
       "4  0.233929  0.226981  0.219453  ...  0.232041  0.251699  0.231547  0.239487   \n",
       "\n",
       "        z16       z17       z18       z19       z20  label  \n",
       "0  0.240368  0.222757  0.217457  0.222375  0.230399      A  \n",
       "1  0.248545  0.227390  0.216762  0.227295  0.241564      A  \n",
       "2  0.241257  0.225991  0.223449  0.227114  0.233759      A  \n",
       "3  0.242942  0.223314  0.217289  0.224675  0.234197      A  \n",
       "4  0.247135  0.224706  0.215417  0.225201  0.238368      A  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"processed_data.json\")\n",
    "# df['label'] = df['label'].replace({'A': 0, 'B': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# le = LabelEncoder()\n",
    "\n",
    "# # Fit and transform your DataFrame's last column\n",
    "# df['label'] = le.fit_transform(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Split the data into features (X) and labels (y)\n",
    "# X = df.iloc[:, :-1]  # First 63 columns as features\n",
    "# y = df['label']  # Last column as the label\n",
    "\n",
    "# # Split the dataset into training set and test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# # Convert to tensors\n",
    "# train_tensors = TensorDataset(torch.tensor(X_train.values, dtype=torch.float), torch.tensor(y_train.values, dtype=torch.long))\n",
    "# test_tensors = TensorDataset(torch.tensor(X_test.values, dtype=torch.float), torch.tensor(y_test.values, dtype=torch.long))\n",
    "\n",
    "# # Create DataLoaders\n",
    "# batch_size = 32  # Choose according to your needs or performance considerations\n",
    "# train_loader = DataLoader(train_tensors, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_tensors, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "# Convert output strings to integers\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "# Split features and target\n",
    "X = df.iloc[:, :63].values  # Features\n",
    "y = df['label'].values  # Target\n",
    "\n",
    "# Split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to float32 for better compatibility with PyTorch\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y).long()  # Long tensor for classification labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/6612 (0%)]\tLoss: 3.180721\n",
      "Train Epoch: 0 [3200/6612 (48%)]\tLoss: 3.147403\n",
      "Train Epoch: 0 [6400/6612 (97%)]\tLoss: 2.978036\n",
      "Train Epoch: 1 [0/6612 (0%)]\tLoss: 2.932304\n",
      "Train Epoch: 1 [3200/6612 (48%)]\tLoss: 2.796500\n",
      "Train Epoch: 1 [6400/6612 (97%)]\tLoss: 2.452566\n",
      "Train Epoch: 2 [0/6612 (0%)]\tLoss: 2.489521\n",
      "Train Epoch: 2 [3200/6612 (48%)]\tLoss: 2.392346\n",
      "Train Epoch: 2 [6400/6612 (97%)]\tLoss: 2.207166\n",
      "Train Epoch: 3 [0/6612 (0%)]\tLoss: 2.013390\n",
      "Train Epoch: 3 [3200/6612 (48%)]\tLoss: 1.849828\n",
      "Train Epoch: 3 [6400/6612 (97%)]\tLoss: 1.876013\n",
      "Train Epoch: 4 [0/6612 (0%)]\tLoss: 1.891037\n",
      "Train Epoch: 4 [3200/6612 (48%)]\tLoss: 1.749223\n",
      "Train Epoch: 4 [6400/6612 (97%)]\tLoss: 1.522879\n",
      "Train Epoch: 5 [0/6612 (0%)]\tLoss: 1.310601\n",
      "Train Epoch: 5 [3200/6612 (48%)]\tLoss: 1.491680\n",
      "Train Epoch: 5 [6400/6612 (97%)]\tLoss: 1.492205\n",
      "Train Epoch: 6 [0/6612 (0%)]\tLoss: 1.482730\n",
      "Train Epoch: 6 [3200/6612 (48%)]\tLoss: 1.011822\n",
      "Train Epoch: 6 [6400/6612 (97%)]\tLoss: 1.216321\n",
      "Train Epoch: 7 [0/6612 (0%)]\tLoss: 1.458319\n",
      "Train Epoch: 7 [3200/6612 (48%)]\tLoss: 1.239582\n",
      "Train Epoch: 7 [6400/6612 (97%)]\tLoss: 0.970049\n",
      "Train Epoch: 8 [0/6612 (0%)]\tLoss: 1.288114\n",
      "Train Epoch: 8 [3200/6612 (48%)]\tLoss: 0.963054\n",
      "Train Epoch: 8 [6400/6612 (97%)]\tLoss: 0.946900\n",
      "Train Epoch: 9 [0/6612 (0%)]\tLoss: 1.039399\n",
      "Train Epoch: 9 [3200/6612 (48%)]\tLoss: 0.904701\n",
      "Train Epoch: 9 [6400/6612 (97%)]\tLoss: 1.005415\n",
      "Train Epoch: 10 [0/6612 (0%)]\tLoss: 1.227701\n",
      "Train Epoch: 10 [3200/6612 (48%)]\tLoss: 0.807646\n",
      "Train Epoch: 10 [6400/6612 (97%)]\tLoss: 0.950833\n",
      "Train Epoch: 11 [0/6612 (0%)]\tLoss: 0.799223\n",
      "Train Epoch: 11 [3200/6612 (48%)]\tLoss: 0.765727\n",
      "Train Epoch: 11 [6400/6612 (97%)]\tLoss: 0.863585\n",
      "Train Epoch: 12 [0/6612 (0%)]\tLoss: 0.796985\n",
      "Train Epoch: 12 [3200/6612 (48%)]\tLoss: 0.883854\n",
      "Train Epoch: 12 [6400/6612 (97%)]\tLoss: 0.823174\n",
      "Train Epoch: 13 [0/6612 (0%)]\tLoss: 0.806330\n",
      "Train Epoch: 13 [3200/6612 (48%)]\tLoss: 0.630436\n",
      "Train Epoch: 13 [6400/6612 (97%)]\tLoss: 0.930521\n",
      "Train Epoch: 14 [0/6612 (0%)]\tLoss: 0.573791\n",
      "Train Epoch: 14 [3200/6612 (48%)]\tLoss: 0.726483\n",
      "Train Epoch: 14 [6400/6612 (97%)]\tLoss: 1.014122\n",
      "Train Epoch: 15 [0/6612 (0%)]\tLoss: 0.602339\n",
      "Train Epoch: 15 [3200/6612 (48%)]\tLoss: 0.729616\n",
      "Train Epoch: 15 [6400/6612 (97%)]\tLoss: 0.547085\n",
      "Train Epoch: 16 [0/6612 (0%)]\tLoss: 0.601702\n",
      "Train Epoch: 16 [3200/6612 (48%)]\tLoss: 0.616434\n",
      "Train Epoch: 16 [6400/6612 (97%)]\tLoss: 0.581588\n",
      "Train Epoch: 17 [0/6612 (0%)]\tLoss: 0.537485\n",
      "Train Epoch: 17 [3200/6612 (48%)]\tLoss: 0.666284\n",
      "Train Epoch: 17 [6400/6612 (97%)]\tLoss: 0.527416\n",
      "Train Epoch: 18 [0/6612 (0%)]\tLoss: 0.463570\n",
      "Train Epoch: 18 [3200/6612 (48%)]\tLoss: 0.573963\n",
      "Train Epoch: 18 [6400/6612 (97%)]\tLoss: 0.713734\n",
      "Train Epoch: 19 [0/6612 (0%)]\tLoss: 0.536255\n",
      "Train Epoch: 19 [3200/6612 (48%)]\tLoss: 0.560625\n",
      "Train Epoch: 19 [6400/6612 (97%)]\tLoss: 0.726373\n",
      "Train Epoch: 20 [0/6612 (0%)]\tLoss: 0.798479\n",
      "Train Epoch: 20 [3200/6612 (48%)]\tLoss: 0.529538\n",
      "Train Epoch: 20 [6400/6612 (97%)]\tLoss: 0.515047\n",
      "Train Epoch: 21 [0/6612 (0%)]\tLoss: 0.523315\n",
      "Train Epoch: 21 [3200/6612 (48%)]\tLoss: 0.311980\n",
      "Train Epoch: 21 [6400/6612 (97%)]\tLoss: 0.278792\n",
      "Train Epoch: 22 [0/6612 (0%)]\tLoss: 0.676931\n",
      "Train Epoch: 22 [3200/6612 (48%)]\tLoss: 0.505979\n",
      "Train Epoch: 22 [6400/6612 (97%)]\tLoss: 0.419116\n",
      "Train Epoch: 23 [0/6612 (0%)]\tLoss: 0.535980\n",
      "Train Epoch: 23 [3200/6612 (48%)]\tLoss: 0.596478\n",
      "Train Epoch: 23 [6400/6612 (97%)]\tLoss: 0.639085\n",
      "Train Epoch: 24 [0/6612 (0%)]\tLoss: 0.450607\n",
      "Train Epoch: 24 [3200/6612 (48%)]\tLoss: 0.803057\n",
      "Train Epoch: 24 [6400/6612 (97%)]\tLoss: 0.429955\n",
      "Train Epoch: 25 [0/6612 (0%)]\tLoss: 0.394152\n",
      "Train Epoch: 25 [3200/6612 (48%)]\tLoss: 0.360133\n",
      "Train Epoch: 25 [6400/6612 (97%)]\tLoss: 0.402493\n",
      "Train Epoch: 26 [0/6612 (0%)]\tLoss: 0.457511\n",
      "Train Epoch: 26 [3200/6612 (48%)]\tLoss: 0.323929\n",
      "Train Epoch: 26 [6400/6612 (97%)]\tLoss: 0.559826\n",
      "Train Epoch: 27 [0/6612 (0%)]\tLoss: 0.540442\n",
      "Train Epoch: 27 [3200/6612 (48%)]\tLoss: 0.423304\n",
      "Train Epoch: 27 [6400/6612 (97%)]\tLoss: 0.848913\n",
      "Train Epoch: 28 [0/6612 (0%)]\tLoss: 0.413335\n",
      "Train Epoch: 28 [3200/6612 (48%)]\tLoss: 0.370922\n",
      "Train Epoch: 28 [6400/6612 (97%)]\tLoss: 0.317578\n",
      "Train Epoch: 29 [0/6612 (0%)]\tLoss: 0.448343\n",
      "Train Epoch: 29 [3200/6612 (48%)]\tLoss: 0.514246\n",
      "Train Epoch: 29 [6400/6612 (97%)]\tLoss: 0.592689\n",
      "Train Epoch: 30 [0/6612 (0%)]\tLoss: 0.356518\n",
      "Train Epoch: 30 [3200/6612 (48%)]\tLoss: 0.264500\n",
      "Train Epoch: 30 [6400/6612 (97%)]\tLoss: 0.277271\n",
      "Train Epoch: 31 [0/6612 (0%)]\tLoss: 0.577530\n",
      "Train Epoch: 31 [3200/6612 (48%)]\tLoss: 0.351537\n",
      "Train Epoch: 31 [6400/6612 (97%)]\tLoss: 0.492261\n",
      "Train Epoch: 32 [0/6612 (0%)]\tLoss: 0.218195\n",
      "Train Epoch: 32 [3200/6612 (48%)]\tLoss: 0.523861\n",
      "Train Epoch: 32 [6400/6612 (97%)]\tLoss: 0.433965\n",
      "Train Epoch: 33 [0/6612 (0%)]\tLoss: 0.345740\n",
      "Train Epoch: 33 [3200/6612 (48%)]\tLoss: 0.732093\n",
      "Train Epoch: 33 [6400/6612 (97%)]\tLoss: 0.312479\n",
      "Train Epoch: 34 [0/6612 (0%)]\tLoss: 0.342780\n",
      "Train Epoch: 34 [3200/6612 (48%)]\tLoss: 0.378688\n",
      "Train Epoch: 34 [6400/6612 (97%)]\tLoss: 0.316763\n",
      "Train Epoch: 35 [0/6612 (0%)]\tLoss: 0.188696\n",
      "Train Epoch: 35 [3200/6612 (48%)]\tLoss: 0.652097\n",
      "Train Epoch: 35 [6400/6612 (97%)]\tLoss: 0.426524\n",
      "Train Epoch: 36 [0/6612 (0%)]\tLoss: 0.466163\n",
      "Train Epoch: 36 [3200/6612 (48%)]\tLoss: 0.178495\n",
      "Train Epoch: 36 [6400/6612 (97%)]\tLoss: 0.295461\n",
      "Train Epoch: 37 [0/6612 (0%)]\tLoss: 0.187747\n",
      "Train Epoch: 37 [3200/6612 (48%)]\tLoss: 0.277729\n",
      "Train Epoch: 37 [6400/6612 (97%)]\tLoss: 0.622939\n",
      "Train Epoch: 38 [0/6612 (0%)]\tLoss: 0.287293\n",
      "Train Epoch: 38 [3200/6612 (48%)]\tLoss: 0.250649\n",
      "Train Epoch: 38 [6400/6612 (97%)]\tLoss: 0.389021\n",
      "Train Epoch: 39 [0/6612 (0%)]\tLoss: 0.574201\n",
      "Train Epoch: 39 [3200/6612 (48%)]\tLoss: 0.272417\n",
      "Train Epoch: 39 [6400/6612 (97%)]\tLoss: 0.323293\n",
      "\n",
      "Test set: Average loss: 0.0116, Accuracy: 1494/1654 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, 40)\n",
    "test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.iloc[0].tolist()[:-1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.argmax(model(torch.Tensor(df.iloc[0].tolist()[:-1])).detach().numpy())\n",
    "# model(torch.Tensor(df.iloc[0].tolist()[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = pd.read_json('test.json').iloc[0].to_list()\n",
    "np.argmax(model(torch.Tensor(inp)).detach().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
